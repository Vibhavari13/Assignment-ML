{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           W         X         Y         Z\n",
      "0   2.706850  0.628133  0.907969  0.503826\n",
      "1   0.651118 -0.319318 -0.848077  0.605965\n",
      "2  -2.018168  0.740122  0.528813 -0.589001\n",
      "3   0.188695 -0.758872 -0.933237  0.955057\n",
      "4   0.190794  1.978757  2.605967  0.683509\n",
      "..       ...       ...       ...       ...\n",
      "95  1.351807  0.257183  0.126086  0.466044\n",
      "96 -0.135813 -0.710667  0.863646 -0.726741\n",
      "97  0.417412  1.273269 -1.194699 -0.145231\n",
      "98  0.013519 -2.156488 -0.494917  0.071770\n",
      "99 -1.291559  1.001002  1.669011  1.480148\n",
      "\n",
      "[100 rows x 4 columns]\n",
      "           W         X         Y         Z  O\n",
      "0   2.706850  0.628133  0.907969  0.503826  0\n",
      "1   0.651118 -0.319318 -0.848077  0.605965  1\n",
      "2  -2.018168  0.740122  0.528813 -0.589001  1\n",
      "3   0.188695 -0.758872 -0.933237  0.955057  0\n",
      "4   0.190794  1.978757  2.605967  0.683509  1\n",
      "..       ...       ...       ...       ... ..\n",
      "95  1.351807  0.257183  0.126086  0.466044  0\n",
      "96 -0.135813 -0.710667  0.863646 -0.726741  1\n",
      "97  0.417412  1.273269 -1.194699 -0.145231  1\n",
      "98  0.013519 -2.156488 -0.494917  0.071770  1\n",
      "99 -1.291559  1.001002  1.669011  1.480148  1\n",
      "\n",
      "[100 rows x 5 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      "W    100 non-null float64\n",
      "X    100 non-null float64\n",
      "Y    100 non-null float64\n",
      "Z    100 non-null float64\n",
      "O    100 non-null int64\n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 4.0 KB\n",
      "           W         X         Y         Z  O\n",
      "34 -0.251897 -0.578120  0.236996  0.200780  0\n",
      "2  -2.018168  0.740122  0.528813 -0.589001  1\n",
      "72  0.230087  0.618658 -2.052563 -0.166646  0\n",
      "37 -0.982776  2.231555 -0.971393 -1.522333  0\n",
      "86  0.307046  0.529927  0.940120  0.746221  1\n",
      "..       ...       ...       ...       ... ..\n",
      "83  0.463428 -2.206442  0.544159  0.698296  0\n",
      "24 -1.972605 -0.866885  0.720788 -1.223082  1\n",
      "23  0.641806 -0.905100 -0.391157  1.028293  1\n",
      "84  1.252944 -1.048347 -1.281320  1.276731  1\n",
      "15  0.386030  2.084019 -0.376519  0.230336  0\n",
      "\n",
      "[80 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.random import randn\n",
    "\n",
    "import random\n",
    "np.random.seed(101)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(randn(100,4),columns='W X Y Z'.split())\n",
    "print(df)\n",
    "\n",
    "ran=list(np.random.randint(2, size=100))\n",
    "df['O']=ran\n",
    "\n",
    "df=df.reset_index(drop=True)\n",
    "print(df)\n",
    "df.info()\n",
    "#split\n",
    "train=df.sample(frac=0.8,random_state=200) #random state is a seed value\n",
    "test=df.drop(train.index)\n",
    "\n",
    "print(train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.2518969516247645, -0.5781202586473068, 0.23699563041333727, 0.20078046900718044, 0.0], [-2.018168244037392, 0.7401220570561068, 0.5288134940893595, -0.5890005332865824, 1.0], [0.23008704402132232, 0.6186579932288591, -2.052562921769063, -0.16664611600310902, 0.0], [-0.982775996932035, 2.231555243446201, -0.9713925659501077, -1.522332873870511, 0.0], [0.3070458805557361, 0.5299269309702093, 0.9401202710995398, 0.7462212187689661, 1.0], [0.7719700393250232, 0.2942166265326114, -1.3915190611130732, 0.14748647106358997, 0.0], [-0.18190368968652595, -0.6032354658427626, -2.552660256955763, -0.031926974302405105, 0.0], [0.09069874396718668, 1.3217153465920728, 0.7941285064681898, 1.3597119775466715, 0.0], [-0.11677331646707445, 1.9017547952605123, 0.23812695876901832, 1.9966522853695665, 0.0], [0.9925734530526948, 1.1922406372754866, -1.0467795376984979, 1.2927645769293514, 0.0], [-0.3801035020513599, -1.666059178570647, -2.7369945956467303, 1.522562110668041, 0.0], [-0.993263499973366, 0.19679950499134005, -1.1366445936091856, 0.000366479605643592, 0.0], [-0.015974763986253394, -1.4331305898343247, -0.7859865892784398, 0.9811694417465484, 1.0], [0.6088572918018318, 0.2864729879605598, -0.2057922982750036, 2.493990185752055, 1.0], [0.7541214332828665, -0.7720406439702567, -0.034431636973305195, 0.6884084244168481, 1.0], [2.6012400932502255, -0.6125519616716804, -1.7149626063052978, 1.350045792334346, 0.0], [-1.40912568010266, 0.8709070168048411, 1.58581239401606, 0.9296578406361021, 0.0], [-0.1253808013891206, -0.9455881247341669, 2.0295441849199127, -1.0463583859222323, 1.0], [1.1300180527493824, 0.897796311118923, 0.33086562044846474, -1.063048893618331, 1.0], [0.1470267713241236, -0.47944803904109595, 0.558769406443067, 1.0248102783372157, 1.0], [-1.3982896782716048, -0.21931071609607042, -0.04567647974128554, 0.012420990255292875, 1.0], [0.6451323806032381, -0.2976370877973264, 0.7557421957582812, 1.7581217444856017, 0.0], [-1.005186917256266, -0.7417897046689249, 0.1871245217641948, -0.7328451475428807, 0.0], [-0.5472598883029545, 0.4967269541347469, -1.235614261466594, -0.7984279607352327, 0.0], [-1.0804714624711786, 0.902397757200862, 0.161781188240493, 0.8330293241751475, 1.0], [1.0040561480091232, 1.0319158600110228, -1.0811182770358438, -0.00035496108900161965, 1.0], [0.3026654485851825, 1.693722925204035, -1.7060859307350775, -1.1591194155484297, 0.0], [-0.8583042112896733, -0.22221024494383712, -0.05120286374238727, -0.4391280231226803, 1.0], [-0.38325813145045406, -0.40931848394745096, 0.3435391865584499, 0.1962753160993944, 1.0], [1.1337029907766036, 0.5281867344521978, 0.3934607601553278, -0.6305066330018959, 1.0], [-1.5010412350670501, 1.0583569254143221, 1.8549671627302513, 0.5022503366851555, 1.0], [-1.420949475967842, 0.3216595852289016, 1.0073241321766129, 0.32978365243716745, 1.0], [1.2902299309450938, -1.4783193644514252, 0.2142341936452114, -0.24050994802670836, 1.0], [-0.8169062469135775, 0.12228660357743058, -1.426758523023901, 1.2604754412328396, 1.0], [0.9757196803436843, -0.38823869993016363, 0.7833160508104224, -0.7089537376785922, 1.0], [1.2599036895486682, -0.4478983519268438, 0.26620677922249886, 0.4125801614290498, 0.0], [0.586847295403904, -1.621347825454474, 0.677535113895638, 0.026105480322676483, 1.0], [0.0936279847307588, 1.2408126131652035, -1.097693023064908, -1.908008821232767, 0.0], [1.606779861236061, -1.1157099674628352, -1.3853785569149153, -1.3296600588247933, 0.0], [-1.599170657601589, -2.0688363569367563, 1.509747011077927, 0.17468320626554876, 0.0], [-0.729466336942255, -0.7201097480739456, -0.28525676642496123, 2.090539149200246, 1.0], [1.407188843201487, 0.060970809221258734, -1.5423927248905593, -0.8211095759183248, 1.0], [0.4174118285256776, 1.2732693537505926, -1.1946993298067712, -0.14523129570790672, 1.0], [-1.3829200981158674, 1.48249549608345, 0.9614581560918355, -2.1412122910809264, 1.0], [0.08152335776533699, -0.41519111613560106, 0.05366145791755185, -2.1151130977123773, 0.0], [0.19079432237171562, 1.9787573241128278, 2.60596727979128, 0.6835088855389145, 1.0], [2.154846443259472, -0.6102588558227414, -0.755325340010558, -0.34641850351854453, 1.0], [-0.9286022737931389, 1.715950166352582, -0.46875628568514655, 0.8600970357844115, 1.0], [0.5527943169790989, -0.1905111479213253, 0.23160916561094283, 0.15541189182413953, 1.0], [-0.14126622522608934, 0.0855288075783365, -1.883808720450088, -1.314142174337265, 0.0], [-1.2915591828336213, 1.0010022005752726, 1.6690108914432338, 1.480147704270954, 1.0], [-0.8548599013156002, 1.6028155369261388, 0.1854789874674731, -0.9940182922196608, 1.0], [-0.49710402288933153, -0.7540697010400628, -0.9434064032519315, 0.48475164718163294, 0.0], [0.3056315115097357, 0.24317761384049238, 0.8641648782095376, -1.5609314472141087, 0.0], [-0.3084717137367063, -0.8420664966420917, -0.8311617565879831, -0.4939413149954647, 1.0], [-2.736464445063238, -0.9851528548352678, -0.1552757504931956, -1.0132280125006923, 1.0], [1.3518070449342112, 0.2571828802021224, 0.126085735824519, 0.4660441456719803, 0.0], [0.18869530944922425, -0.758872056210466, -0.9332372163009188, 0.9550565092637361, 0.0], [2.706849839399938, 0.6281327087844596, 0.9079694464765431, 0.5038257538223936, 0.0], [0.9028258592331454, -0.5379091569515423, -1.5496711043545173, 0.43525292201184523, 0.0], [2.165421117012678, -1.2672401758233451, -0.8028431714973141, -0.25347938278107685, 1.0], [0.39248881107944394, 0.22149068500354543, -0.8551960407780934, 1.541990408031232, 1.0], [-0.27951021452358354, 1.0627104161410734, 1.7520144226756424, 0.6955467245457797, 1.0], [-1.4675140203361003, -0.49409535833912277, -0.1625347347726149, 0.48580873745486103, 0.0], [-0.5508760030855312, 0.942044977340326, -0.9753493500596645, -1.0548505975739644, 0.0], [-0.21988154841914037, 0.2927951308453091, 0.8343545216262672, -0.14043183108525809, 1.0], [-0.13190613678378701, -1.3350917346306141, -0.0892984417103869, 0.6985664041829249, 0.0], [-1.3064823050884957, -1.632923996816882, 0.8021293698822711, -1.6675367808286456, 0.0], [0.19752398574797547, 2.3029872653509056, 0.7290237321354455, -0.8630912690663913, 0.0], [-1.6782836579965759, 0.33397310118137624, -0.5324705255224986, 2.1177273820227196, 0.0], [0.3278447402697301, 0.674485186989649, -0.17405654125505843, 0.7801397150660992, 1.0], [-1.0874358897562575, -2.567766921174143, 0.6610291979302003, -0.33292080038923216, 1.0], [0.013519410019218173, -2.156487652428678, -0.4949174704085828, 0.07176997133324095, 1.0], [0.9887728450784197, 0.5138327290356798, -0.9282048316241881, 0.8469038980594052, 1.0], [0.6663193209198146, -0.5382346255173922, -0.5685813606701186, 1.4073382479609386, 1.0], [0.4634275027319352, -2.20644204251316, 0.5441592393194943, 0.6982957338773823, 0.0], [-1.9726051036093835, -0.8668850353083267, 0.7207875987587121, -1.2230820379046312, 1.0], [0.6418055114450044, -0.905099901810439, -0.3911566271858832, 1.0282931608187034, 1.0], [1.2529436328908892, -1.0483473351311796, -1.2813202935443193, 1.2767314501588805, 1.0], [0.3860303121135517, 2.084018530338962, -0.37651867524923904, 0.23033634359240704, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "l=[]\n",
    "for i in train.values:\n",
    "    l.append(list(i))\n",
    "    \n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.500, error=38.000\n",
      ">epoch=1, lrate=0.500, error=37.000\n",
      ">epoch=2, lrate=0.500, error=41.000\n",
      ">epoch=3, lrate=0.500, error=37.000\n",
      ">epoch=4, lrate=0.500, error=34.000\n",
      ">epoch=0, lrate=0.010, error=26.000\n",
      ">epoch=1, lrate=0.010, error=32.000\n",
      ">epoch=2, lrate=0.010, error=28.000\n",
      ">epoch=3, lrate=0.010, error=29.000\n",
      ">epoch=4, lrate=0.010, error=28.000\n",
      ">epoch=5, lrate=0.010, error=27.000\n",
      ">epoch=6, lrate=0.010, error=25.000\n",
      ">epoch=7, lrate=0.010, error=26.000\n",
      ">epoch=8, lrate=0.010, error=24.000\n",
      ">epoch=9, lrate=0.010, error=25.000\n",
      ">epoch=10, lrate=0.010, error=23.000\n",
      ">epoch=11, lrate=0.010, error=27.000\n",
      ">epoch=12, lrate=0.010, error=24.000\n",
      ">epoch=13, lrate=0.010, error=24.000\n",
      ">epoch=14, lrate=0.010, error=29.000\n",
      ">epoch=15, lrate=0.010, error=25.000\n",
      ">epoch=16, lrate=0.010, error=28.000\n",
      ">epoch=17, lrate=0.010, error=23.000\n",
      ">epoch=18, lrate=0.010, error=32.000\n",
      ">epoch=19, lrate=0.010, error=28.000\n",
      ">epoch=20, lrate=0.010, error=24.000\n",
      ">epoch=21, lrate=0.010, error=25.000\n",
      ">epoch=22, lrate=0.010, error=23.000\n",
      ">epoch=23, lrate=0.010, error=26.000\n",
      ">epoch=24, lrate=0.010, error=26.000\n",
      ">epoch=25, lrate=0.010, error=26.000\n",
      ">epoch=26, lrate=0.010, error=28.000\n",
      ">epoch=27, lrate=0.010, error=27.000\n",
      ">epoch=28, lrate=0.010, error=28.000\n",
      ">epoch=29, lrate=0.010, error=25.000\n",
      ">epoch=30, lrate=0.010, error=25.000\n",
      ">epoch=31, lrate=0.010, error=26.000\n",
      ">epoch=32, lrate=0.010, error=27.000\n",
      ">epoch=33, lrate=0.010, error=20.000\n",
      ">epoch=34, lrate=0.010, error=24.000\n",
      ">epoch=35, lrate=0.010, error=26.000\n",
      ">epoch=36, lrate=0.010, error=30.000\n",
      ">epoch=37, lrate=0.010, error=27.000\n",
      ">epoch=38, lrate=0.010, error=26.000\n",
      ">epoch=39, lrate=0.010, error=23.000\n",
      ">epoch=40, lrate=0.010, error=29.000\n",
      ">epoch=41, lrate=0.010, error=24.000\n",
      ">epoch=42, lrate=0.010, error=24.000\n",
      ">epoch=43, lrate=0.010, error=26.000\n",
      ">epoch=44, lrate=0.010, error=27.000\n",
      ">epoch=45, lrate=0.010, error=22.000\n",
      ">epoch=46, lrate=0.010, error=24.000\n",
      ">epoch=47, lrate=0.010, error=30.000\n",
      ">epoch=48, lrate=0.010, error=27.000\n",
      ">epoch=49, lrate=0.010, error=26.000\n",
      ">epoch=50, lrate=0.010, error=31.000\n",
      ">epoch=51, lrate=0.010, error=25.000\n",
      ">epoch=52, lrate=0.010, error=26.000\n",
      ">epoch=53, lrate=0.010, error=29.000\n",
      ">epoch=54, lrate=0.010, error=26.000\n",
      ">epoch=55, lrate=0.010, error=27.000\n",
      ">epoch=56, lrate=0.010, error=26.000\n",
      ">epoch=57, lrate=0.010, error=24.000\n",
      ">epoch=58, lrate=0.010, error=26.000\n",
      ">epoch=59, lrate=0.010, error=27.000\n",
      ">epoch=60, lrate=0.010, error=24.000\n",
      ">epoch=61, lrate=0.010, error=30.000\n",
      ">epoch=62, lrate=0.010, error=24.000\n",
      ">epoch=63, lrate=0.010, error=27.000\n",
      ">epoch=64, lrate=0.010, error=26.000\n",
      ">epoch=65, lrate=0.010, error=26.000\n",
      ">epoch=66, lrate=0.010, error=26.000\n",
      ">epoch=67, lrate=0.010, error=23.000\n",
      ">epoch=68, lrate=0.010, error=25.000\n",
      ">epoch=69, lrate=0.010, error=23.000\n",
      ">epoch=70, lrate=0.010, error=23.000\n",
      ">epoch=71, lrate=0.010, error=24.000\n",
      ">epoch=72, lrate=0.010, error=23.000\n",
      ">epoch=73, lrate=0.010, error=25.000\n",
      ">epoch=74, lrate=0.010, error=28.000\n",
      ">epoch=75, lrate=0.010, error=23.000\n",
      ">epoch=76, lrate=0.010, error=27.000\n",
      ">epoch=77, lrate=0.010, error=26.000\n",
      ">epoch=78, lrate=0.010, error=26.000\n",
      ">epoch=79, lrate=0.010, error=24.000\n",
      ">epoch=80, lrate=0.010, error=24.000\n",
      ">epoch=81, lrate=0.010, error=28.000\n",
      ">epoch=82, lrate=0.010, error=25.000\n",
      ">epoch=83, lrate=0.010, error=23.000\n",
      ">epoch=84, lrate=0.010, error=25.000\n",
      ">epoch=85, lrate=0.010, error=28.000\n",
      ">epoch=86, lrate=0.010, error=27.000\n",
      ">epoch=87, lrate=0.010, error=19.000\n",
      ">epoch=88, lrate=0.010, error=24.000\n",
      ">epoch=89, lrate=0.010, error=27.000\n",
      ">epoch=90, lrate=0.010, error=29.000\n",
      ">epoch=91, lrate=0.010, error=29.000\n",
      ">epoch=92, lrate=0.010, error=26.000\n",
      ">epoch=93, lrate=0.010, error=22.000\n",
      ">epoch=94, lrate=0.010, error=24.000\n",
      ">epoch=95, lrate=0.010, error=23.000\n",
      ">epoch=96, lrate=0.010, error=23.000\n",
      ">epoch=97, lrate=0.010, error=25.000\n",
      ">epoch=98, lrate=0.010, error=28.000\n",
      ">epoch=99, lrate=0.010, error=24.000\n",
      "-------Fold 1 ------\n",
      "*****Hyperparameters*****\n",
      "Cumulative Epochs:  5000\n",
      "Learning rate:  0.02 \n",
      "\n",
      "METRICS\n",
      "Accuracy:  0.5384615384615384\n",
      "Confusion Matrix:\n",
      "11  2\n",
      "10  3\n",
      "Precision:  0.5238095238095238\n",
      "Recall:  0.8461538461538461 \n",
      "\n",
      ">epoch=0, lrate=0.010, error=24.000\n",
      ">epoch=1, lrate=0.010, error=21.000\n",
      ">epoch=2, lrate=0.010, error=23.000\n",
      ">epoch=3, lrate=0.010, error=21.000\n",
      ">epoch=4, lrate=0.010, error=25.000\n",
      ">epoch=5, lrate=0.010, error=25.000\n",
      ">epoch=6, lrate=0.010, error=20.000\n",
      ">epoch=7, lrate=0.010, error=19.000\n",
      ">epoch=8, lrate=0.010, error=23.000\n",
      ">epoch=9, lrate=0.010, error=22.000\n",
      ">epoch=10, lrate=0.010, error=14.000\n",
      ">epoch=11, lrate=0.010, error=15.000\n",
      ">epoch=12, lrate=0.010, error=17.000\n",
      ">epoch=13, lrate=0.010, error=23.000\n",
      ">epoch=14, lrate=0.010, error=20.000\n",
      ">epoch=15, lrate=0.010, error=24.000\n",
      ">epoch=16, lrate=0.010, error=23.000\n",
      ">epoch=17, lrate=0.010, error=20.000\n",
      ">epoch=18, lrate=0.010, error=17.000\n",
      ">epoch=19, lrate=0.010, error=22.000\n",
      ">epoch=20, lrate=0.010, error=19.000\n",
      ">epoch=21, lrate=0.010, error=21.000\n",
      ">epoch=22, lrate=0.010, error=23.000\n",
      ">epoch=23, lrate=0.010, error=23.000\n",
      ">epoch=24, lrate=0.010, error=22.000\n",
      ">epoch=25, lrate=0.010, error=23.000\n",
      ">epoch=26, lrate=0.010, error=21.000\n",
      ">epoch=27, lrate=0.010, error=21.000\n",
      ">epoch=28, lrate=0.010, error=25.000\n",
      ">epoch=29, lrate=0.010, error=21.000\n",
      ">epoch=30, lrate=0.010, error=19.000\n",
      ">epoch=31, lrate=0.010, error=25.000\n",
      ">epoch=32, lrate=0.010, error=20.000\n",
      ">epoch=33, lrate=0.010, error=20.000\n",
      ">epoch=34, lrate=0.010, error=22.000\n",
      ">epoch=35, lrate=0.010, error=23.000\n",
      ">epoch=36, lrate=0.010, error=20.000\n",
      ">epoch=37, lrate=0.010, error=24.000\n",
      ">epoch=38, lrate=0.010, error=20.000\n",
      ">epoch=39, lrate=0.010, error=21.000\n",
      ">epoch=40, lrate=0.010, error=20.000\n",
      ">epoch=41, lrate=0.010, error=16.000\n",
      ">epoch=42, lrate=0.010, error=22.000\n",
      ">epoch=43, lrate=0.010, error=25.000\n",
      ">epoch=44, lrate=0.010, error=28.000\n",
      ">epoch=45, lrate=0.010, error=20.000\n",
      ">epoch=46, lrate=0.010, error=21.000\n",
      ">epoch=47, lrate=0.010, error=23.000\n",
      ">epoch=48, lrate=0.010, error=23.000\n",
      ">epoch=49, lrate=0.010, error=25.000\n",
      ">epoch=50, lrate=0.010, error=17.000\n",
      ">epoch=51, lrate=0.010, error=27.000\n",
      ">epoch=52, lrate=0.010, error=18.000\n",
      ">epoch=53, lrate=0.010, error=22.000\n",
      ">epoch=54, lrate=0.010, error=22.000\n",
      ">epoch=55, lrate=0.010, error=21.000\n",
      ">epoch=56, lrate=0.010, error=23.000\n",
      ">epoch=57, lrate=0.010, error=21.000\n",
      ">epoch=58, lrate=0.010, error=17.000\n",
      ">epoch=59, lrate=0.010, error=21.000\n",
      ">epoch=60, lrate=0.010, error=24.000\n",
      ">epoch=61, lrate=0.010, error=23.000\n",
      ">epoch=62, lrate=0.010, error=26.000\n",
      ">epoch=63, lrate=0.010, error=19.000\n",
      ">epoch=64, lrate=0.010, error=22.000\n",
      ">epoch=65, lrate=0.010, error=23.000\n",
      ">epoch=66, lrate=0.010, error=22.000\n",
      ">epoch=67, lrate=0.010, error=22.000\n",
      ">epoch=68, lrate=0.010, error=22.000\n",
      ">epoch=69, lrate=0.010, error=18.000\n",
      ">epoch=70, lrate=0.010, error=19.000\n",
      ">epoch=71, lrate=0.010, error=18.000\n",
      ">epoch=72, lrate=0.010, error=19.000\n",
      ">epoch=73, lrate=0.010, error=23.000\n",
      ">epoch=74, lrate=0.010, error=23.000\n",
      ">epoch=75, lrate=0.010, error=26.000\n",
      ">epoch=76, lrate=0.010, error=23.000\n",
      ">epoch=77, lrate=0.010, error=23.000\n",
      ">epoch=78, lrate=0.010, error=24.000\n",
      ">epoch=79, lrate=0.010, error=23.000\n",
      ">epoch=80, lrate=0.010, error=24.000\n",
      ">epoch=81, lrate=0.010, error=20.000\n",
      ">epoch=82, lrate=0.010, error=22.000\n",
      ">epoch=83, lrate=0.010, error=27.000\n",
      ">epoch=84, lrate=0.010, error=23.000\n",
      ">epoch=85, lrate=0.010, error=21.000\n",
      ">epoch=86, lrate=0.010, error=23.000\n",
      ">epoch=87, lrate=0.010, error=21.000\n",
      ">epoch=88, lrate=0.010, error=22.000\n",
      ">epoch=89, lrate=0.010, error=26.000\n",
      ">epoch=90, lrate=0.010, error=22.000\n",
      ">epoch=91, lrate=0.010, error=22.000\n",
      ">epoch=92, lrate=0.010, error=19.000\n",
      ">epoch=93, lrate=0.010, error=19.000\n",
      ">epoch=94, lrate=0.010, error=22.000\n",
      ">epoch=95, lrate=0.010, error=23.000\n",
      ">epoch=96, lrate=0.010, error=22.000\n",
      ">epoch=97, lrate=0.010, error=20.000\n",
      ">epoch=98, lrate=0.010, error=21.000\n",
      ">epoch=99, lrate=0.010, error=24.000\n",
      "-------Fold 2 ------\n",
      "*****Hyperparameters*****\n",
      "Cumulative Epochs:  10000\n",
      "Learning rate:  0.02 \n",
      "\n",
      "METRICS\n",
      "Accuracy:  0.4230769230769231\n",
      "Confusion Matrix:\n",
      "10  5\n",
      "10  1\n",
      "Precision:  0.5\n",
      "Recall:  0.6666666666666666 \n",
      "\n",
      ">epoch=0, lrate=0.010, error=24.000\n",
      ">epoch=1, lrate=0.010, error=24.000\n",
      ">epoch=2, lrate=0.010, error=19.000\n",
      ">epoch=3, lrate=0.010, error=24.000\n",
      ">epoch=4, lrate=0.010, error=24.000\n",
      ">epoch=5, lrate=0.010, error=20.000\n",
      ">epoch=6, lrate=0.010, error=21.000\n",
      ">epoch=7, lrate=0.010, error=25.000\n",
      ">epoch=8, lrate=0.010, error=20.000\n",
      ">epoch=9, lrate=0.010, error=20.000\n",
      ">epoch=10, lrate=0.010, error=23.000\n",
      ">epoch=11, lrate=0.010, error=20.000\n",
      ">epoch=12, lrate=0.010, error=22.000\n",
      ">epoch=13, lrate=0.010, error=21.000\n",
      ">epoch=14, lrate=0.010, error=21.000\n",
      ">epoch=15, lrate=0.010, error=18.000\n",
      ">epoch=16, lrate=0.010, error=25.000\n",
      ">epoch=17, lrate=0.010, error=17.000\n",
      ">epoch=18, lrate=0.010, error=25.000\n",
      ">epoch=19, lrate=0.010, error=23.000\n",
      ">epoch=20, lrate=0.010, error=26.000\n",
      ">epoch=21, lrate=0.010, error=17.000\n",
      ">epoch=22, lrate=0.010, error=23.000\n",
      ">epoch=23, lrate=0.010, error=21.000\n",
      ">epoch=24, lrate=0.010, error=22.000\n",
      ">epoch=25, lrate=0.010, error=23.000\n",
      ">epoch=26, lrate=0.010, error=21.000\n",
      ">epoch=27, lrate=0.010, error=27.000\n",
      ">epoch=28, lrate=0.010, error=20.000\n",
      ">epoch=29, lrate=0.010, error=22.000\n",
      ">epoch=30, lrate=0.010, error=20.000\n",
      ">epoch=31, lrate=0.010, error=21.000\n",
      ">epoch=32, lrate=0.010, error=25.000\n",
      ">epoch=33, lrate=0.010, error=20.000\n",
      ">epoch=34, lrate=0.010, error=25.000\n",
      ">epoch=35, lrate=0.010, error=20.000\n",
      ">epoch=36, lrate=0.010, error=21.000\n",
      ">epoch=37, lrate=0.010, error=26.000\n",
      ">epoch=38, lrate=0.010, error=19.000\n",
      ">epoch=39, lrate=0.010, error=23.000\n",
      ">epoch=40, lrate=0.010, error=24.000\n",
      ">epoch=41, lrate=0.010, error=20.000\n",
      ">epoch=42, lrate=0.010, error=22.000\n",
      ">epoch=43, lrate=0.010, error=22.000\n",
      ">epoch=44, lrate=0.010, error=22.000\n",
      ">epoch=45, lrate=0.010, error=22.000\n",
      ">epoch=46, lrate=0.010, error=23.000\n",
      ">epoch=47, lrate=0.010, error=22.000\n",
      ">epoch=48, lrate=0.010, error=23.000\n",
      ">epoch=49, lrate=0.010, error=22.000\n",
      ">epoch=50, lrate=0.010, error=20.000\n",
      ">epoch=51, lrate=0.010, error=22.000\n",
      ">epoch=52, lrate=0.010, error=23.000\n",
      ">epoch=53, lrate=0.010, error=25.000\n",
      ">epoch=54, lrate=0.010, error=22.000\n",
      ">epoch=55, lrate=0.010, error=23.000\n",
      ">epoch=56, lrate=0.010, error=20.000\n",
      ">epoch=57, lrate=0.010, error=18.000\n",
      ">epoch=58, lrate=0.010, error=23.000\n",
      ">epoch=59, lrate=0.010, error=20.000\n",
      ">epoch=60, lrate=0.010, error=23.000\n",
      ">epoch=61, lrate=0.010, error=17.000\n",
      ">epoch=62, lrate=0.010, error=26.000\n",
      ">epoch=63, lrate=0.010, error=20.000\n",
      ">epoch=64, lrate=0.010, error=16.000\n",
      ">epoch=65, lrate=0.010, error=23.000\n",
      ">epoch=66, lrate=0.010, error=20.000\n",
      ">epoch=67, lrate=0.010, error=21.000\n",
      ">epoch=68, lrate=0.010, error=18.000\n",
      ">epoch=69, lrate=0.010, error=23.000\n",
      ">epoch=70, lrate=0.010, error=23.000\n",
      ">epoch=71, lrate=0.010, error=21.000\n",
      ">epoch=72, lrate=0.010, error=24.000\n",
      ">epoch=73, lrate=0.010, error=23.000\n",
      ">epoch=74, lrate=0.010, error=22.000\n",
      ">epoch=75, lrate=0.010, error=25.000\n",
      ">epoch=76, lrate=0.010, error=21.000\n",
      ">epoch=77, lrate=0.010, error=22.000\n",
      ">epoch=78, lrate=0.010, error=22.000\n",
      ">epoch=79, lrate=0.010, error=21.000\n",
      ">epoch=80, lrate=0.010, error=23.000\n",
      ">epoch=81, lrate=0.010, error=22.000\n",
      ">epoch=82, lrate=0.010, error=21.000\n",
      ">epoch=83, lrate=0.010, error=25.000\n",
      ">epoch=84, lrate=0.010, error=25.000\n",
      ">epoch=85, lrate=0.010, error=24.000\n",
      ">epoch=86, lrate=0.010, error=23.000\n",
      ">epoch=87, lrate=0.010, error=23.000\n",
      ">epoch=88, lrate=0.010, error=22.000\n",
      ">epoch=89, lrate=0.010, error=23.000\n",
      ">epoch=90, lrate=0.010, error=23.000\n",
      ">epoch=91, lrate=0.010, error=23.000\n",
      ">epoch=92, lrate=0.010, error=24.000\n",
      ">epoch=93, lrate=0.010, error=20.000\n",
      ">epoch=94, lrate=0.010, error=22.000\n",
      ">epoch=95, lrate=0.010, error=23.000\n",
      ">epoch=96, lrate=0.010, error=22.000\n",
      ">epoch=97, lrate=0.010, error=20.000\n",
      ">epoch=98, lrate=0.010, error=21.000\n",
      ">epoch=99, lrate=0.010, error=27.000\n",
      "-------Fold 3 ------\n",
      "*****Hyperparameters*****\n",
      "Cumulative Epochs:  15000\n",
      "Learning rate:  0.02 \n",
      "\n",
      "METRICS\n",
      "Accuracy:  0.5384615384615384\n",
      "Confusion Matrix:\n",
      "11  5\n",
      "7  3\n",
      "Precision:  0.6111111111111112\n",
      "Recall:  0.6875 \n",
      "\n",
      "Scores: [0.5384615384615384, 0.4230769230769231, 0.5384615384615384]\n",
      "Mean Accuracy: 0.500%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b7fb374240b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# Split a dataset into k folds\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "error_graph_data=[]\n",
    "stor_weights=[]\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "\tweights = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0.0\n",
    "\t\tfor row in train:\n",
    "\t\t\tprediction = predict(row, weights)\n",
    "\t\t\terror = row[-1] - prediction\n",
    "\t\t\tsum_error += error**2\n",
    "\t\t\tweights[0] = weights[0] + l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\treturn weights\n",
    " \n",
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "\tactivation = weights[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tactivation += weights[i + 1] * row[i]\n",
    "\treturn 1.0 if activation >= 0.0 else 0.0\n",
    "\n",
    "\n",
    "def perceptron(train, test, l_rate, n_epoch):\n",
    "\tpredictions = list()\n",
    "\tweights = train_weights(train, l_rate, n_epoch)\n",
    "\tfor row in test:\n",
    "\t\tprediction = predict(row, weights)\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn(predictions) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "  \n",
    "def find_acc_metrics(act, pred):\n",
    "    tn,tp,fn,fp,= 0,0,0,0 \n",
    "    #true negatve, true positive, false negative, false positive\n",
    "    # 1=True, 0=False\n",
    "    for i in range(len(act)): \n",
    "        if act[i] == 1 and pred[i]==1: \n",
    "            tp+=1\n",
    "        elif act[i]==1 and pred[i]==0:\n",
    "            fn+=1\n",
    "        elif act[i]==0 and pred[i]==1:\n",
    "            fp+=1\n",
    "        else: \n",
    "            tn+=1\n",
    "    #Accuracy, Confusion Matrix, Precision, Recall\n",
    "    return [(tn+tp)/(tn+tp+fn+fp),str(tp)+'  '+str(fn)+'\\n'+str(fp)+'  '+str(tn),tp/(tp+fp),tp/(tp+fn)]\n",
    " \n",
    "\n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "\n",
    "\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "        \n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "    folds = cross_validation_split(dataset, n_folds) \n",
    "    #get folds\n",
    "    scores = list() \n",
    "    #list of accuracies\n",
    "    for i in range(len(folds)): \n",
    "        #for every fold\n",
    "        train_set = list(folds) \n",
    "        #your training set will be all the folds minus the current fold\n",
    "        train_set.remove(folds[i]) \n",
    "        #remove current fold \n",
    "        train_set = sum(train_set, []) \n",
    "        test_set = list() \n",
    "        #empty test set\n",
    "        actual=list()\n",
    "        for row in folds[i]:\n",
    "            row_copy = list(row) \n",
    "            #get entire row in the fold\n",
    "            actual.append(row_copy[-1]) \n",
    "            #append the true value\n",
    "            row_copy[-1] = None \n",
    "            #set the to predict attribute to \"None\"\n",
    "            test_set.append(row_copy) \n",
    "            #append it into the test set\n",
    "        \n",
    "        predicted = algorithm(train_set, test_set, *args) \n",
    "        #get predictions from the MLP algorithm\n",
    "        metrics = find_acc_metrics(actual, predicted) \n",
    "        #calculate the metrics\n",
    "        print('-------Fold',i+1,'------')\n",
    "        print('*****Hyperparameters*****') #print the hyperparameters\n",
    "        print('Cumulative Epochs: ',5000*(i+1)) #print results after nth epoch\n",
    "        print('Learning rate: ',0.02,'\\n') #alpha \n",
    "        print('METRICS')\n",
    "        print('Accuracy: ',metrics[0]) \n",
    "        print('Confusion Matrix:\\n'+metrics[1])\n",
    "        print('Precision: ',metrics[2])\n",
    "        print('Recall: ',metrics[3],'\\n')\n",
    "        scores.append(metrics[0]) #append into scores\n",
    "    return scores\n",
    "\n",
    "\t\n",
    " \n",
    "# Perceptron Algorithm With Stochastic Gradient Descent\n",
    "\n",
    " \n",
    "# Test the Perceptron algorithm on the sonar dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "\n",
    "\n",
    "l_rate = 0.5\n",
    "n_epoch = 5\n",
    "train_weights(l, l_rate, n_epoch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_folds = 3\n",
    "l_rate = 0.01\n",
    "n_epoch = 100\n",
    "scores = evaluate_algorithm(l, perceptron, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' %scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
    "\n",
    "    \n",
    "for row in l:\n",
    "    print(predict(row,weights))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
